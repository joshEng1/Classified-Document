PORT=5055
MAX_UPLOAD_MB=25
CORS_ORIGINS=http://localhost:5055,http://127.0.0.1:5055
CORS_ALLOW_NULL=false
CORS_ALLOW_ALL=false
OPENAI_API_KEY=sk-...
VERIFIER_ENGINE=llama # one of: openai, llama (llama recommended for Windows GPU)
# Windows native: use localhost for GPU server running on host
# llama.cpp model servers
LLAMA_URL=http://localhost:8080 # Primary LLM server (classifier/verifier)
# Optional: separate SLM and Guardian model servers
SLM_URL=http://localhost:8083    # Summarization LLM (optional)
GUARDIAN_URL=http://localhost:8081 # Granite Guardian moderation
GUARDIAN_MODEL=granite-guardian-3.2-3b-a800m
# Optional: Granite Vision (multimodal) server
VISION_URL=http://localhost:8082
VISION_MODEL=granite-vision-3.2-2b-Q5_K_M.gguf
VISION_ENABLE=true
VISION_IMAGE_COVERAGE_THRESHOLD=0.25
VISION_FIGURE_COUNT_THRESHOLD=1
VISION_MIN_TEXT_CHARS_WITH_FIGURES=200
VISION_CROP_FIGURES=true
VISION_MAX_REGIONS_PER_PAGE=3
VISION_MIN_REGION_AREA_PCT=0.03
VISION_MIN_TOTAL_REGION_AREA_PCT=0.15
VISION_RENDER_DPI=220
VISION_MAX_PAGES=12
# If running server in Docker, use: http://host.docker.internal:8080
DOCLING_URL=http://localhost:7000 # optional docling REST endpoint
ALLOW_BATCH_PATHS=false # set true to allow /api/process-batch {"paths":[...]} input (restricted to BATCH_PATH_ROOTS)
BATCH_PATH_ROOTS= # optional comma-separated absolute roots allowed for batch paths; default is assets/documents and uploads
ROUTE_LOW=0.5
AUTO_ACCEPT=0.9
LOCAL_CLASSIFIER=llama # one of: heuristic, llama (llama recommended with GPU)
VERIFY_SECOND_PASS=true # if true, always run verifier after local classification
LOCAL_DEFAULT_CONF=0.92 # default confidence to assign to llama local classification
REDACT_PII=true # redact PII before guard checks/verifier
REDACT_OUTPUT_PDF=true # produce a redacted PDF with burned-in boxes (PII + flagged figure regions)
CROSS_VERIFY=false # use double-layer validation (llama + openai if available)
OFFLINE_MODE=true # when true, avoids any external network calls by default
LLM_MODEL_NAME=granite-4.0-350m-IQ4_NL.gguf # optional: reported in results
LLM_TEMPERATURE=0 # optional: overrides llama/SLM temperature (0 = deterministic)
GUARDIAN_DEBUG=false # optional: verbose guardian request/response logging
GUARDIAN_FLAG_THRESHOLD=0.35 # optional: lower = stricter moderation (default 0.5)
