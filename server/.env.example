PORT=5055
MAX_UPLOAD_MB=25
CORS_ORIGINS=http://localhost:5055,http://127.0.0.1:5055,https://your-portfolio-domain.com
CORS_ALLOW_NULL=false
CORS_ALLOW_ALL=false
TRUST_PROXY=true # set true when deployed behind Vercel/Render/Nginx/Cloudflare proxy
PUBLIC_API_RATE_LIMIT_ENABLED=true
PUBLIC_API_RATE_LIMIT_WINDOW_MS=60000
PUBLIC_API_RATE_LIMIT_MAX_REQUESTS=8
PUBLIC_API_RATE_LIMIT_METHODS=POST
PUBLIC_API_RATE_LIMIT_PATHS=/api/process,/api/process-stream,/api/process-batch,/api/pdf/session,/api/pdf/render-pages,/api/pdf/redact
API_NO_STORE=true # add Cache-Control: no-store on /api responses
VERBOSE_SERVER_LOGS=false # keep false in production to avoid noisy/sensitive logs
LLAMA_DEBUG=false # debug logs for local llama verifier; keep false for public deployments
UPLOAD_RETENTION_MINUTES=120 # cleanup TTL for uploads/sessions/redacted artifacts
UPLOAD_CLEANUP_INTERVAL_MINUTES=15 # periodic cleanup interval

# Hosted model mode (recommended for public portfolio use)
MODEL_MODE_DEFAULT=online # one of: online, local (offline is treated as local)
ONLINE_PROVIDER=gemini # one of: gemini, openai
GEMINI_API_KEY=
GEMINI_MODEL=gemini-3-flash-preview
GEMINI_SUMMARY_MODEL=gemini-3-flash-preview
GEMINI_MODERATION_MODEL=gemini-3-flash-preview
GEMINI_PII_MODEL=gemini-3-flash-preview
GEMINI_TIMEOUT_MS=45000
API_KEY= # optional legacy fallback for Gemini key
GOOGLE_VISION_API_KEY= # optional override; defaults to GEMINI_API_KEY/API_KEY for Cloud Vision quick image scan
GOOGLE_CLOUD_ACCESS_TOKEN= # optional shared OAuth token for Google APIs (Vision + Document AI)
DOC_AI_ACCESS_TOKEN= # optional dedicated OAuth token for Document AI (takes precedence over GOOGLE_CLOUD_ACCESS_TOKEN)
DOC_AI_PROJECT_ID=
DOC_AI_LOCATION=us
DOC_AI_PROCESSOR_ID= # Document AI OCR processor id
DOC_AI_TIMEOUT_MS=90000
CLOUD_VISION_SAMPLE_MAX_PAGES=2
CLOUD_VISION_SAMPLE_DPI=120
CLOUD_VISION_OBJECT_THRESHOLD=0.55
CLOUD_VISION_TIMEOUT_MS=20000
OPENAI_API_KEY= # optional: used when ONLINE_PROVIDER=openai (or legacy fallback for Gemini key)
OPENAI_MODEL=gpt-5.2 # used only when ONLINE_PROVIDER=openai
VERIFIER_ENGINE=openai # set llama to force local mode by default; non-llama implies online mode
OFFLINE_MODE=false # must be false when using hosted APIs
LOCAL_CLASSIFIER=heuristic # one of: heuristic, llama
VERIFY_SECOND_PASS=true # keep true to preserve verifier + citation workflow
CROSS_VERIFY=false # optional dual-verifier mode (online provider + llama)

# Optional local llama.cpp fallback
# Windows native: use localhost for GPU server running on host
LLAMA_URL=http://localhost:8080 # Primary LLM server (classifier/verifier)
LLM_MODEL_NAME=granite-4.0-350m-IQ4_NL.gguf # reported in results when llama is used
LLM_TEMPERATURE=0 # optional: overrides llama/SLM temperature (0 = deterministic)
LOCAL_DEFAULT_CONF=0.92 # default confidence to assign to llama local classification

# Optional: separate SLM and Guardian model servers
SLM_URL=http://localhost:8083    # Summarization LLM (optional)
GUARDIAN_URL=http://localhost:8081 # Granite Guardian moderation
GUARDIAN_MODEL=granite-guardian-3.2-3b-a800m
# Optional: Granite Vision (multimodal) server
VISION_URL=http://localhost:8082
VISION_MODEL=granite-vision-3.2-2b-Q5_K_M.gguf
VISION_ENABLE=true
VISION_IMAGE_COVERAGE_THRESHOLD=0.25
VISION_FIGURE_COUNT_THRESHOLD=1
VISION_MIN_TEXT_CHARS_WITH_FIGURES=200
VISION_CROP_FIGURES=true
VISION_MAX_REGIONS_PER_PAGE=3
VISION_MIN_REGION_AREA_PCT=0.03
VISION_MIN_TOTAL_REGION_AREA_PCT=0.15
VISION_RENDER_DPI=220
VISION_MAX_PAGES=12
# If running server in Docker, use: http://host.docker.internal:8080
DOCLING_URL=http://localhost:7000 # optional docling REST endpoint
ALLOW_BATCH_PATHS=false # set true to allow /api/process-batch {"paths":[...]} input (restricted to BATCH_PATH_ROOTS)
BATCH_PATH_ROOTS= # optional comma-separated absolute roots allowed for batch paths; default is assets/documents and uploads
ROUTE_LOW=0.5
AUTO_ACCEPT=0.9
REDACT_PII=true # redact PII before guard checks/verifier
REDACT_OUTPUT_PDF=true # produce a redacted PDF with burned-in boxes (PII + flagged figure regions)
GUARDIAN_DEBUG=false # optional: verbose guardian request/response logging
GUARDIAN_FLAG_THRESHOLD=0.35 # optional: lower = stricter moderation (default 0.5)
