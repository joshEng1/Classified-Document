PORT=5055
OPENAI_API_KEY=sk-...
VERIFIER_ENGINE=llama # one of: openai, llama (llama recommended for Windows GPU)
# Windows native: use localhost for GPU server running on host
# llama.cpp model servers
LLAMA_URL=http://localhost:8080 # Primary LLM server (classifier/verifier)
# Optional: separate SLM and Guardian model servers
SLM_URL=http://localhost:8080    # Summarization LLM (Granite-4.0 350M)
GUARDIAN_URL=http://localhost:8081 # Granite Guardian moderation
# If running server in Docker, use: http://host.docker.internal:8080
DOCLING_URL=http://localhost:7000 # optional docling REST endpoint
ROUTE_LOW=0.5
AUTO_ACCEPT=0.9
LOCAL_CLASSIFIER=llama # one of: heuristic, llama (llama recommended with GPU)
VERIFY_SECOND_PASS=true # if true, always run verifier after local classification
LOCAL_DEFAULT_CONF=0.92 # default confidence to assign to llama local classification
REDACT_PII=true # redact PII before guard checks/verifier
CROSS_VERIFY=false # use double-layer validation (llama + openai if available)
OFFLINE_MODE=true # when true, avoids any external network calls by default
LLM_MODEL_NAME=granite-4.0-350m-IQ4_NL.gguf # optional: reported in results
